Documentation
----------------
     In this directory, the various files present in this folder set up the Elastic Container Service (ECS) cluster by creating an instance of it with several other Amazon Web Services (AWS) resources such as the infrastructure provider, IAM user account roles, a Virtual Private Cloud, security groups, a Docker image to run on the ECS, an application load balancer (ALB) to regulate internet traffic passing through the established network, multiple variables to provide a certain level of functionality within the system, and so forth. These resources are split amongst a series of interconnected Terraform scripts which are titled based on their primary functions. They also work together in tandem to provide the user with access to the specified resources in the AWS console. For each resource that is specified in the code, a detailed listing is formed within an associated area of the cloud environment. 

     As for the functionality of the code, it will compile correctly in its current state and perform the specified operations. The files present in the directory have not been edited from their source material since they already seem to fit the objective of establishing an ECS instance within the us-west-2 region (Oregon). At first, implementing this code within the terminal window of my Ubuntu 18 virtual machine seemed to repeatedly and consistently compile successfully, but none of the specified infrastructure was actually showing up in the AWS console. This was odd since there were no errors thrown and managing the code with the "$ terraform plan", "$ terraform apply", and "$ terraform destroy" commands did not appear to affect the compilation process. There were no immediately identifiable flags to inspect that would describe the necessary adjustments to be made as to get the infrastructure running. I do not know what was causing this specific problem, but I changed some aspects of my own AWS account and resynced the Terraform environment with the "$ aws configure" command". These actions may or may not have had an impact on getting the code to run properly. 

     For the file structure and overall hierarchy, there are fifteen Terraform scripts and a singe tpl file stored in the templates subdirectory to build the specified infrastructure. The code in the scripts are used to setup the necessary resources in AWS, while the tpl extension indicates a template file that includes style data and other information required to create a specific resource. These files work together within the same directory and require each other to operate correctly. Based on multiple online articles, it seems that this version of ECS setup can utilize a scalable number of tasks when integrated appropriately with other code. An alternative way of specifying the ECS would be to use Elastic Compute Cloud (EC2) instances (web servers) instead of adding the Fargate feature. This is because the other way appears to focus on various segmented resources or tasks within several EC2 instances to perfom its operations and function properly, whereas ECS implemented with Fargate incorporates various system-related tasks directly within the ECS container.  

     When it comes to this type of configuration, AWS Fargate runs the ECS container directly without the need for any EC2 instances. "AWS Fargate manages the task execution which means that there are no EC2 instances to manage anymore. You only pay for the running tasks and thatâ€™s it. It is as easy as it sounds. Each task that runs in Fargate comes with a dedicated Elastic Network Interface (ENI) with a private IP address. All containers of the same task can communicate with each other via localhost. Inbound and outbound task communication goes through the ENI. A public IP address can be enabled as well."