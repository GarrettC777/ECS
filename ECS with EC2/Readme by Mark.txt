Documentation
----------------
     In this directory, the various files present in this folder set up the Elastic Container Service (ECS) cluster by creating an instance of it with several other Amazon Web Services (AWS) resources such as the infrastructure provider, IAM user account roles, a Virtual Private Cloud, security groups, the latest Amazon Machine Image (AMI) for the ECS, the Nginx service, an application load balancer (ALB) to regulate internet traffic passing through the established network, multiple variables to provide a certain level of functionality within the system, and so forth. These resources are split amongst a series of interconnected Terraform scripts which are titled based on their primary functions. They also work together in tandem to provide the user with access to the specified resources in the AWS console. For each resource that is specified in the code, a detailed listing is formed within an associated area of the cloud environment. 

     As for the functionality of the code, it will compile correctly in its current state and perform the specified operations. Some of the files have been changed to better reflect the nature of the project by modifying the region field listed in certain areas from eu-west-2 (London) to us-west-2 (Oregon). This is why there might be a number of references to the aformentioned region in the specified code. It has been adapted to fit a closer geographical area in the hopes of making it operate more efficiently and cost effectively. At first, implementing this code within the terminal window of my Ubuntu 18 virtual machine seemed to repeatedly and consistently compile successfully, but none of the specified infrastructure was actually showing up in the AWS console. This was odd since there were no errors thrown and managing the code with the "$ terraform plan", "$ terraform apply", and "$ terraform destroy" commands did not appear to affect the compilation process. There were no immediately identifiable flags to inspect that would describe the necessary adjustments to be made as to get the infrastructure running. I do not know what was causing this specific problem, but I changed some aspects of my own AWS account and resynced the Terraform environment with the "$ aws configure" command". These actions may or may not have had an impact on getting the code to run properly. 

     For the file structure and overall hierarchy, there are nine Terraform scripts and a singe tpl file to build the specified infrastructure. The code in the scripts are used to setup the necessary resources in AWS, while the tpl extension indicates a template file that includes style data and other information required to create a specific resource. These files work together within the same directory and require each other to operate correctly. Based on multiple online articles, it seems that this version of ECS setup can utilize a scalable number of Elastic Compute Cloud (EC2) instances (web servers) when integrated appropriately with other code. An alternative way of specifying the ECS would be to use the Fargate feature instead of adding EC2 instances. This is because the other way appears to incorporate various system-related tasks directly within the ECS container instead of focusing on various segmented resources or tasks within several EC2 instances to perfom its operations and function properly. 

     When it comes to this type of setup, it deploys, runs, and manages your cluster of EC2 instances indirectly within the ECS container. "An ECS container instance is nothing more than an EC2 instance that runs the ECS Container Agent. The EC2 instance is owned and managed by you. The instance appears in the list of EC2 instances like any other EC2 instance. The ECS Container Agent regularly polls the ECS API if new containers need to be started or stopped. Usually, you run a cluster of container instances in an auto-scaling group. ECS is free of charge. You only pay for the EC2 instances. The downside is that you have to scale, monitor, patch, and secure the EC2 instances yourself. Especially the scaling is not easy because: 

          * There is no obvious metric to scale the cluster and no integration to scale when the task placement fails because of insufficient capacity.

          * The auto-scaling group and ECS are not aware of each other which makes task deployments very hard during cluster scale in or rolling updates via                CloudFormation. 
           
          * You have to scale down without killing running tasks which is an even more significant challenge for long lived tasks."



